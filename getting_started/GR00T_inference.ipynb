{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GR00T Inference\n",
    "\n",
    "This tutorial shows how to use the GR00T inference model to predict the actions from the observations, given a test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leju/CodeBase/VLA/Isaac-Tang-GR00T/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import gr00t\n",
    "\n",
    "from gr00t.data.dataset.lerobot_episode_loader import LeRobotEpisodeLoader\n",
    "from gr00t.data.dataset.sharded_single_step_dataset import extract_step_data\n",
    "from gr00t.data.embodiment_tags import EmbodimentTag\n",
    "from gr00t.policy.gr00t_policy import Gr00tPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the following paths\n",
    "MODEL_PATH = \"nvidia/GR00T-N1.6-3B\"\n",
    "\n",
    "# REPO_PATH is the path of the pip install gr00t repo and one level up\n",
    "REPO_PATH = os.path.dirname(os.path.dirname(gr00t.__file__))\n",
    "DATASET_PATH = os.path.join(REPO_PATH, \"demo_data/gr1.PickNPlace\")\n",
    "EMBODIMENT_TAG = \"gr1\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Pretrained Policy\n",
    "\n",
    "Policy Model is loaded just like any other huggingface model.\n",
    "\n",
    "There are 2 new concepts here in the GR00T model:\n",
    " - modality config: This defines the keys in the dictionary used by the model. (e.g. `action`, `state`, `annotation`, `video`)\n",
    " - modality_transform: A sequence of transform which are used during dataloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tune backbone llm: False\n",
      "Tune backbone visual: False\n",
      "Backbone trainable parameter: model.language_model.model.layers.12.self_attn.q_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.12.self_attn.k_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.12.self_attn.v_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.12.self_attn.o_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.12.self_attn.q_norm.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.12.self_attn.k_norm.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.12.mlp.gate_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.12.mlp.up_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.12.mlp.down_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.12.input_layernorm.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.12.post_attention_layernorm.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.13.self_attn.q_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.13.self_attn.k_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.13.self_attn.v_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.13.self_attn.o_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.13.self_attn.q_norm.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.13.self_attn.k_norm.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.13.mlp.gate_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.13.mlp.up_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.13.mlp.down_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.13.input_layernorm.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.13.post_attention_layernorm.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.14.self_attn.q_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.14.self_attn.k_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.14.self_attn.v_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.14.self_attn.o_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.14.self_attn.q_norm.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.14.self_attn.k_norm.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.14.mlp.gate_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.14.mlp.up_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.14.mlp.down_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.14.input_layernorm.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.14.post_attention_layernorm.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.15.self_attn.q_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.15.self_attn.k_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.15.self_attn.v_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.15.self_attn.o_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.15.self_attn.q_norm.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.15.self_attn.k_norm.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.15.mlp.gate_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.15.mlp.up_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.15.mlp.down_proj.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.15.input_layernorm.weight\n",
      "Backbone trainable parameter: model.language_model.model.layers.15.post_attention_layernorm.weight\n",
      "Casting trainable parameter model.language_model.model.layers.12.self_attn.q_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.12.self_attn.k_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.12.self_attn.v_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.12.self_attn.o_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.12.self_attn.q_norm.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.12.self_attn.k_norm.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.12.mlp.gate_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.12.mlp.up_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.12.mlp.down_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.12.input_layernorm.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.12.post_attention_layernorm.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.13.self_attn.q_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.13.self_attn.k_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.13.self_attn.v_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.13.self_attn.o_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.13.self_attn.q_norm.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.13.self_attn.k_norm.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.13.mlp.gate_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.13.mlp.up_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.13.mlp.down_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.13.input_layernorm.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.13.post_attention_layernorm.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.14.self_attn.q_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.14.self_attn.k_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.14.self_attn.v_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.14.self_attn.o_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.14.self_attn.q_norm.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.14.self_attn.k_norm.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.14.mlp.gate_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.14.mlp.up_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.14.mlp.down_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.14.input_layernorm.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.14.post_attention_layernorm.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.15.self_attn.q_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.15.self_attn.k_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.15.self_attn.v_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.15.self_attn.o_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.15.self_attn.q_norm.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.15.self_attn.k_norm.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.15.mlp.gate_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.15.mlp.up_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.15.mlp.down_proj.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.15.input_layernorm.weight to fp32\n",
      "Casting trainable parameter model.language_model.model.layers.15.post_attention_layernorm.weight to fp32\n",
      "Total number of DiT parameters:  1091722240\n",
      "Using AlternateVLDiT for diffusion model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leju/CodeBase/VLA/Isaac-Tang-GR00T/gr00t/model/modules/dit.py:205: FutureWarning: Accessing config attribute `compute_dtype` directly via 'AlternateVLDiT' object attribute is deprecated. Please access 'compute_dtype' over 'AlternateVLDiT's config object instead, e.g. 'unet.config.compute_dtype'.\n",
      "  embedding_dim=self.inner_dim, compute_dtype=self.compute_dtype\n",
      "/home/leju/CodeBase/VLA/Isaac-Tang-GR00T/gr00t/model/modules/dit.py:236: FutureWarning: Accessing config attribute `output_dim` directly via 'AlternateVLDiT' object attribute is deprecated. Please access 'output_dim' over 'AlternateVLDiT's config object instead, e.g. 'unet.config.output_dim'.\n",
      "  self.proj_out_2 = nn.Linear(self.inner_dim, self.output_dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tune action head projector: True\n",
      "Tune action head diffusion model: True\n",
      "Tune action head vlln: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gr00tN1d6(\n",
      "  (backbone): EagleBackbone(\n",
      "    (model): Eagle3_VLForConditionalGeneration(\n",
      "      (vision_model): Siglip2VisionModel(\n",
      "        (vision_model): Siglip2VisionTransformer(\n",
      "          (embeddings): Siglip2VisionEmbeddings(\n",
      "            (patch_embedding): Linear(in_features=588, out_features=1152, bias=True)\n",
      "            (position_embedding): Embedding(256, 1152)\n",
      "          )\n",
      "          (encoder): Siglip2Encoder(\n",
      "            (rope_2d): Rope2DPosEmb(dim=72, max_height=512, max_width=512, theta_base=14)\n",
      "            (layers): ModuleList(\n",
      "              (0-26): 27 x Siglip2EncoderLayer(\n",
      "                (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "                (self_attn): Siglip2Attention(\n",
      "                  (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "                  (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "                  (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "                  (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "                )\n",
      "                (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "                (mlp): Siglip2MLP(\n",
      "                  (activation_fn): PytorchGELUTanh()\n",
      "                  (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "                  (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "          (head): Siglip2MultiheadAttentionPoolingHead(\n",
      "            (attention): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1152, out_features=1152, bias=True)\n",
      "            )\n",
      "            (layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Siglip2MLP(\n",
      "              (activation_fn): PytorchGELUTanh()\n",
      "              (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "              (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (language_model): Qwen3ForCausalLM(\n",
      "        (model): Qwen3Model(\n",
      "          (embed_tokens): Embedding(151680, 2048)\n",
      "          (layers): ModuleList(\n",
      "            (0-15): 16 x Qwen3DecoderLayer(\n",
      "              (self_attn): Qwen3Attention(\n",
      "                (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "                (k_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "                (v_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "                (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "                (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "                (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "              )\n",
      "              (mlp): Qwen3MLP(\n",
      "                (gate_proj): Linear(in_features=2048, out_features=6144, bias=False)\n",
      "                (up_proj): Linear(in_features=2048, out_features=6144, bias=False)\n",
      "                (down_proj): Linear(in_features=6144, out_features=2048, bias=False)\n",
      "                (act_fn): SiLU()\n",
      "              )\n",
      "              (input_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "              (post_attention_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "            )\n",
      "          )\n",
      "          (norm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
      "          (rotary_emb): Qwen3RotaryEmbedding()\n",
      "        )\n",
      "        (lm_head): Linear(in_features=2048, out_features=151680, bias=False)\n",
      "      )\n",
      "      (mlp1): Sequential(\n",
      "        (0): LayerNorm((4608,), eps=1e-05, elementwise_affine=True)\n",
      "        (1): Linear(in_features=4608, out_features=2048, bias=True)\n",
      "        (2): GELU(approximate='none')\n",
      "        (3): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (action_head): Gr00tN1d6ActionHead(\n",
      "    (model): AlternateVLDiT(\n",
      "      (timestep_encoder): TimestepEncoder(\n",
      "        (time_proj): Timesteps()\n",
      "        (timestep_embedder): TimestepEmbedding(\n",
      "          (linear_1): Linear(in_features=256, out_features=1536, bias=True)\n",
      "          (act): SiLU()\n",
      "          (linear_2): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (transformer_blocks): ModuleList(\n",
      "        (0): BasicTransformerBlock(\n",
      "          (norm1): AdaLayerNorm(\n",
      "            (silu): SiLU()\n",
      "            (linear): Linear(in_features=1536, out_features=3072, bias=True)\n",
      "            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
      "          )\n",
      "          (attn1): Attention(\n",
      "            (to_q): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (to_k): Linear(in_features=2048, out_features=1536, bias=True)\n",
      "            (to_v): Linear(in_features=2048, out_features=1536, bias=True)\n",
      "            (to_out): ModuleList(\n",
      "              (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "              (1): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
      "          (ff): FeedForward(\n",
      "            (net): ModuleList(\n",
      "              (0): GELU(\n",
      "                (proj): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "              )\n",
      "              (1): Dropout(p=0.2, inplace=False)\n",
      "              (2): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "              (3): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (final_dropout): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "        (1): BasicTransformerBlock(\n",
      "          (norm1): AdaLayerNorm(\n",
      "            (silu): SiLU()\n",
      "            (linear): Linear(in_features=1536, out_features=3072, bias=True)\n",
      "            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
      "          )\n",
      "          (attn1): Attention(\n",
      "            (to_q): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (to_k): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (to_v): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (to_out): ModuleList(\n",
      "              (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "              (1): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
      "          (ff): FeedForward(\n",
      "            (net): ModuleList(\n",
      "              (0): GELU(\n",
      "                (proj): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "              )\n",
      "              (1): Dropout(p=0.2, inplace=False)\n",
      "              (2): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "              (3): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (final_dropout): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "        (2): BasicTransformerBlock(\n",
      "          (norm1): AdaLayerNorm(\n",
      "            (silu): SiLU()\n",
      "            (linear): Linear(in_features=1536, out_features=3072, bias=True)\n",
      "            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
      "          )\n",
      "          (attn1): Attention(\n",
      "            (to_q): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (to_k): Linear(in_features=2048, out_features=1536, bias=True)\n",
      "            (to_v): Linear(in_features=2048, out_features=1536, bias=True)\n",
      "            (to_out): ModuleList(\n",
      "              (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "              (1): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
      "          (ff): FeedForward(\n",
      "            (net): ModuleList(\n",
      "              (0): GELU(\n",
      "                (proj): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "              )\n",
      "              (1): Dropout(p=0.2, inplace=False)\n",
      "              (2): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "              (3): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (final_dropout): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "        (3): BasicTransformerBlock(\n",
      "          (norm1): AdaLayerNorm(\n",
      "            (silu): SiLU()\n",
      "            (linear): Linear(in_features=1536, out_features=3072, bias=True)\n",
      "            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
      "          )\n",
      "          (attn1): Attention(\n",
      "            (to_q): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (to_k): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (to_v): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (to_out): ModuleList(\n",
      "              (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "              (1): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
      "          (ff): FeedForward(\n",
      "            (net): ModuleList(\n",
      "              (0): GELU(\n",
      "                (proj): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "              )\n",
      "              (1): Dropout(p=0.2, inplace=False)\n",
      "              (2): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "              (3): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (final_dropout): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "        (4): BasicTransformerBlock(\n",
      "          (norm1): AdaLayerNorm(\n",
      "            (silu): SiLU()\n",
      "            (linear): Linear(in_features=1536, out_features=3072, bias=True)\n",
      "            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
      "          )\n",
      "          (attn1): Attention(\n",
      "            (to_q): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (to_k): Linear(in_features=2048, out_features=1536, bias=True)\n",
      "            (to_v): Linear(in_features=2048, out_features=1536, bias=True)\n",
      "            (to_out): ModuleList(\n",
      "              (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "              (1): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
      "          (ff): FeedForward(\n",
      "            (net): ModuleList(\n",
      "              (0): GELU(\n",
      "                (proj): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "              )\n",
      "              (1): Dropout(p=0.2, inplace=False)\n",
      "              (2): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "              (3): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (final_dropout): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "        (5): BasicTransformerBlock(\n",
      "          (norm1): AdaLayerNorm(\n",
      "            (silu): SiLU()\n",
      "            (linear): Linear(in_features=1536, out_features=3072, bias=True)\n",
      "            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
      "          )\n",
      "          (attn1): Attention(\n",
      "            (to_q): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (to_k): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (to_v): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (to_out): ModuleList(\n",
      "              (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "              (1): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
      "          (ff): FeedForward(\n",
      "            (net): ModuleList(\n",
      "              (0): GELU(\n",
      "                (proj): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "              )\n",
      "              (1): Dropout(p=0.2, inplace=False)\n",
      "              (2): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "              (3): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (final_dropout): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "        (6): BasicTransformerBlock(\n",
      "          (norm1): AdaLayerNorm(\n",
      "            (silu): SiLU()\n",
      "            (linear): Linear(in_features=1536, out_features=3072, bias=True)\n",
      "            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
      "          )\n",
      "          (attn1): Attention(\n",
      "            (to_q): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (to_k): Linear(in_features=2048, out_features=1536, bias=True)\n",
      "            (to_v): Linear(in_features=2048, out_features=1536, bias=True)\n",
      "            (to_out): ModuleList(\n",
      "              (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "              (1): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
      "          (ff): FeedForward(\n",
      "            (net): ModuleList(\n",
      "              (0): GELU(\n",
      "                (proj): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "              )\n",
      "              (1): Dropout(p=0.2, inplace=False)\n",
      "              (2): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "              (3): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (final_dropout): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "        (7): BasicTransformerBlock(\n",
      "          (norm1): AdaLayerNorm(\n",
      "            (silu): SiLU()\n",
      "            (linear): Linear(in_features=1536, out_features=3072, bias=True)\n",
      "            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
      "          )\n",
      "          (attn1): Attention(\n",
      "            (to_q): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (to_k): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (to_v): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (to_out): ModuleList(\n",
      "              (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "              (1): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
      "          (ff): FeedForward(\n",
      "            (net): ModuleList(\n",
      "              (0): GELU(\n",
      "                (proj): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "              )\n",
      "              (1): Dropout(p=0.2, inplace=False)\n",
      "              (2): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "              (3): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (final_dropout): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "        (8): BasicTransformerBlock(\n",
      "          (norm1): AdaLayerNorm(\n",
      "            (silu): SiLU()\n",
      "            (linear): Linear(in_features=1536, out_features=3072, bias=True)\n",
      "            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
      "          )\n",
      "          (attn1): Attention(\n",
      "            (to_q): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (to_k): Linear(in_features=2048, out_features=1536, bias=True)\n",
      "            (to_v): Linear(in_features=2048, out_features=1536, bias=True)\n",
      "            (to_out): ModuleList(\n",
      "              (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "              (1): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
      "          (ff): FeedForward(\n",
      "            (net): ModuleList(\n",
      "              (0): GELU(\n",
      "                (proj): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "              )\n",
      "              (1): Dropout(p=0.2, inplace=False)\n",
      "              (2): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "              (3): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (final_dropout): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "        (9): BasicTransformerBlock(\n",
      "          (norm1): AdaLayerNorm(\n",
      "            (silu): SiLU()\n",
      "            (linear): Linear(in_features=1536, out_features=3072, bias=True)\n",
      "            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
      "          )\n",
      "          (attn1): Attention(\n",
      "            (to_q): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (to_k): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (to_v): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (to_out): ModuleList(\n",
      "              (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "              (1): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
      "          (ff): FeedForward(\n",
      "            (net): ModuleList(\n",
      "              (0): GELU(\n",
      "                (proj): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "              )\n",
      "              (1): Dropout(p=0.2, inplace=False)\n",
      "              (2): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "              (3): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (final_dropout): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "        (10): BasicTransformerBlock(\n",
      "          (norm1): AdaLayerNorm(\n",
      "            (silu): SiLU()\n",
      "            (linear): Linear(in_features=1536, out_features=3072, bias=True)\n",
      "            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
      "          )\n",
      "          (attn1): Attention(\n",
      "            (to_q): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (to_k): Linear(in_features=2048, out_features=1536, bias=True)\n",
      "            (to_v): Linear(in_features=2048, out_features=1536, bias=True)\n",
      "            (to_out): ModuleList(\n",
      "              (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "              (1): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
      "          (ff): FeedForward(\n",
      "            (net): ModuleList(\n",
      "              (0): GELU(\n",
      "                (proj): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "              )\n",
      "              (1): Dropout(p=0.2, inplace=False)\n",
      "              (2): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "              (3): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (final_dropout): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "        (11): BasicTransformerBlock(\n",
      "          (norm1): AdaLayerNorm(\n",
      "            (silu): SiLU()\n",
      "            (linear): Linear(in_features=1536, out_features=3072, bias=True)\n",
      "            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
      "          )\n",
      "          (attn1): Attention(\n",
      "            (to_q): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (to_k): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (to_v): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (to_out): ModuleList(\n",
      "              (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "              (1): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
      "          (ff): FeedForward(\n",
      "            (net): ModuleList(\n",
      "              (0): GELU(\n",
      "                (proj): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "              )\n",
      "              (1): Dropout(p=0.2, inplace=False)\n",
      "              (2): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "              (3): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (final_dropout): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "        (12): BasicTransformerBlock(\n",
      "          (norm1): AdaLayerNorm(\n",
      "            (silu): SiLU()\n",
      "            (linear): Linear(in_features=1536, out_features=3072, bias=True)\n",
      "            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
      "          )\n",
      "          (attn1): Attention(\n",
      "            (to_q): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (to_k): Linear(in_features=2048, out_features=1536, bias=True)\n",
      "            (to_v): Linear(in_features=2048, out_features=1536, bias=True)\n",
      "            (to_out): ModuleList(\n",
      "              (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "              (1): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
      "          (ff): FeedForward(\n",
      "            (net): ModuleList(\n",
      "              (0): GELU(\n",
      "                (proj): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "              )\n",
      "              (1): Dropout(p=0.2, inplace=False)\n",
      "              (2): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "              (3): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (final_dropout): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "        (13): BasicTransformerBlock(\n",
      "          (norm1): AdaLayerNorm(\n",
      "            (silu): SiLU()\n",
      "            (linear): Linear(in_features=1536, out_features=3072, bias=True)\n",
      "            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
      "          )\n",
      "          (attn1): Attention(\n",
      "            (to_q): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (to_k): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (to_v): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (to_out): ModuleList(\n",
      "              (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "              (1): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
      "          (ff): FeedForward(\n",
      "            (net): ModuleList(\n",
      "              (0): GELU(\n",
      "                (proj): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "              )\n",
      "              (1): Dropout(p=0.2, inplace=False)\n",
      "              (2): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "              (3): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (final_dropout): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "        (14): BasicTransformerBlock(\n",
      "          (norm1): AdaLayerNorm(\n",
      "            (silu): SiLU()\n",
      "            (linear): Linear(in_features=1536, out_features=3072, bias=True)\n",
      "            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
      "          )\n",
      "          (attn1): Attention(\n",
      "            (to_q): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (to_k): Linear(in_features=2048, out_features=1536, bias=True)\n",
      "            (to_v): Linear(in_features=2048, out_features=1536, bias=True)\n",
      "            (to_out): ModuleList(\n",
      "              (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "              (1): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
      "          (ff): FeedForward(\n",
      "            (net): ModuleList(\n",
      "              (0): GELU(\n",
      "                (proj): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "              )\n",
      "              (1): Dropout(p=0.2, inplace=False)\n",
      "              (2): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "              (3): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (final_dropout): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "        (15): BasicTransformerBlock(\n",
      "          (norm1): AdaLayerNorm(\n",
      "            (silu): SiLU()\n",
      "            (linear): Linear(in_features=1536, out_features=3072, bias=True)\n",
      "            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
      "          )\n",
      "          (attn1): Attention(\n",
      "            (to_q): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (to_k): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (to_v): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (to_out): ModuleList(\n",
      "              (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "              (1): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
      "          (ff): FeedForward(\n",
      "            (net): ModuleList(\n",
      "              (0): GELU(\n",
      "                (proj): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "              )\n",
      "              (1): Dropout(p=0.2, inplace=False)\n",
      "              (2): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "              (3): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (final_dropout): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "        (16): BasicTransformerBlock(\n",
      "          (norm1): AdaLayerNorm(\n",
      "            (silu): SiLU()\n",
      "            (linear): Linear(in_features=1536, out_features=3072, bias=True)\n",
      "            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
      "          )\n",
      "          (attn1): Attention(\n",
      "            (to_q): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (to_k): Linear(in_features=2048, out_features=1536, bias=True)\n",
      "            (to_v): Linear(in_features=2048, out_features=1536, bias=True)\n",
      "            (to_out): ModuleList(\n",
      "              (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "              (1): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
      "          (ff): FeedForward(\n",
      "            (net): ModuleList(\n",
      "              (0): GELU(\n",
      "                (proj): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "              )\n",
      "              (1): Dropout(p=0.2, inplace=False)\n",
      "              (2): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "              (3): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (final_dropout): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "        (17): BasicTransformerBlock(\n",
      "          (norm1): AdaLayerNorm(\n",
      "            (silu): SiLU()\n",
      "            (linear): Linear(in_features=1536, out_features=3072, bias=True)\n",
      "            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
      "          )\n",
      "          (attn1): Attention(\n",
      "            (to_q): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (to_k): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (to_v): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (to_out): ModuleList(\n",
      "              (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "              (1): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
      "          (ff): FeedForward(\n",
      "            (net): ModuleList(\n",
      "              (0): GELU(\n",
      "                (proj): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "              )\n",
      "              (1): Dropout(p=0.2, inplace=False)\n",
      "              (2): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "              (3): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (final_dropout): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "        (18): BasicTransformerBlock(\n",
      "          (norm1): AdaLayerNorm(\n",
      "            (silu): SiLU()\n",
      "            (linear): Linear(in_features=1536, out_features=3072, bias=True)\n",
      "            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
      "          )\n",
      "          (attn1): Attention(\n",
      "            (to_q): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (to_k): Linear(in_features=2048, out_features=1536, bias=True)\n",
      "            (to_v): Linear(in_features=2048, out_features=1536, bias=True)\n",
      "            (to_out): ModuleList(\n",
      "              (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "              (1): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
      "          (ff): FeedForward(\n",
      "            (net): ModuleList(\n",
      "              (0): GELU(\n",
      "                (proj): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "              )\n",
      "              (1): Dropout(p=0.2, inplace=False)\n",
      "              (2): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "              (3): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (final_dropout): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "        (19): BasicTransformerBlock(\n",
      "          (norm1): AdaLayerNorm(\n",
      "            (silu): SiLU()\n",
      "            (linear): Linear(in_features=1536, out_features=3072, bias=True)\n",
      "            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
      "          )\n",
      "          (attn1): Attention(\n",
      "            (to_q): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (to_k): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (to_v): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (to_out): ModuleList(\n",
      "              (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "              (1): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
      "          (ff): FeedForward(\n",
      "            (net): ModuleList(\n",
      "              (0): GELU(\n",
      "                (proj): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "              )\n",
      "              (1): Dropout(p=0.2, inplace=False)\n",
      "              (2): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "              (3): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (final_dropout): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "        (20): BasicTransformerBlock(\n",
      "          (norm1): AdaLayerNorm(\n",
      "            (silu): SiLU()\n",
      "            (linear): Linear(in_features=1536, out_features=3072, bias=True)\n",
      "            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
      "          )\n",
      "          (attn1): Attention(\n",
      "            (to_q): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (to_k): Linear(in_features=2048, out_features=1536, bias=True)\n",
      "            (to_v): Linear(in_features=2048, out_features=1536, bias=True)\n",
      "            (to_out): ModuleList(\n",
      "              (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "              (1): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
      "          (ff): FeedForward(\n",
      "            (net): ModuleList(\n",
      "              (0): GELU(\n",
      "                (proj): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "              )\n",
      "              (1): Dropout(p=0.2, inplace=False)\n",
      "              (2): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "              (3): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (final_dropout): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "        (21): BasicTransformerBlock(\n",
      "          (norm1): AdaLayerNorm(\n",
      "            (silu): SiLU()\n",
      "            (linear): Linear(in_features=1536, out_features=3072, bias=True)\n",
      "            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
      "          )\n",
      "          (attn1): Attention(\n",
      "            (to_q): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (to_k): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (to_v): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (to_out): ModuleList(\n",
      "              (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "              (1): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
      "          (ff): FeedForward(\n",
      "            (net): ModuleList(\n",
      "              (0): GELU(\n",
      "                (proj): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "              )\n",
      "              (1): Dropout(p=0.2, inplace=False)\n",
      "              (2): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "              (3): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (final_dropout): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "        (22): BasicTransformerBlock(\n",
      "          (norm1): AdaLayerNorm(\n",
      "            (silu): SiLU()\n",
      "            (linear): Linear(in_features=1536, out_features=3072, bias=True)\n",
      "            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
      "          )\n",
      "          (attn1): Attention(\n",
      "            (to_q): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (to_k): Linear(in_features=2048, out_features=1536, bias=True)\n",
      "            (to_v): Linear(in_features=2048, out_features=1536, bias=True)\n",
      "            (to_out): ModuleList(\n",
      "              (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "              (1): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
      "          (ff): FeedForward(\n",
      "            (net): ModuleList(\n",
      "              (0): GELU(\n",
      "                (proj): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "              )\n",
      "              (1): Dropout(p=0.2, inplace=False)\n",
      "              (2): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "              (3): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (final_dropout): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "        (23): BasicTransformerBlock(\n",
      "          (norm1): AdaLayerNorm(\n",
      "            (silu): SiLU()\n",
      "            (linear): Linear(in_features=1536, out_features=3072, bias=True)\n",
      "            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
      "          )\n",
      "          (attn1): Attention(\n",
      "            (to_q): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (to_k): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (to_v): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (to_out): ModuleList(\n",
      "              (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "              (1): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
      "          (ff): FeedForward(\n",
      "            (net): ModuleList(\n",
      "              (0): GELU(\n",
      "                (proj): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "              )\n",
      "              (1): Dropout(p=0.2, inplace=False)\n",
      "              (2): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "              (3): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (final_dropout): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "        (24): BasicTransformerBlock(\n",
      "          (norm1): AdaLayerNorm(\n",
      "            (silu): SiLU()\n",
      "            (linear): Linear(in_features=1536, out_features=3072, bias=True)\n",
      "            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
      "          )\n",
      "          (attn1): Attention(\n",
      "            (to_q): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (to_k): Linear(in_features=2048, out_features=1536, bias=True)\n",
      "            (to_v): Linear(in_features=2048, out_features=1536, bias=True)\n",
      "            (to_out): ModuleList(\n",
      "              (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "              (1): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
      "          (ff): FeedForward(\n",
      "            (net): ModuleList(\n",
      "              (0): GELU(\n",
      "                (proj): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "              )\n",
      "              (1): Dropout(p=0.2, inplace=False)\n",
      "              (2): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "              (3): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (final_dropout): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "        (25): BasicTransformerBlock(\n",
      "          (norm1): AdaLayerNorm(\n",
      "            (silu): SiLU()\n",
      "            (linear): Linear(in_features=1536, out_features=3072, bias=True)\n",
      "            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
      "          )\n",
      "          (attn1): Attention(\n",
      "            (to_q): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (to_k): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (to_v): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (to_out): ModuleList(\n",
      "              (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "              (1): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
      "          (ff): FeedForward(\n",
      "            (net): ModuleList(\n",
      "              (0): GELU(\n",
      "                (proj): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "              )\n",
      "              (1): Dropout(p=0.2, inplace=False)\n",
      "              (2): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "              (3): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (final_dropout): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "        (26): BasicTransformerBlock(\n",
      "          (norm1): AdaLayerNorm(\n",
      "            (silu): SiLU()\n",
      "            (linear): Linear(in_features=1536, out_features=3072, bias=True)\n",
      "            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
      "          )\n",
      "          (attn1): Attention(\n",
      "            (to_q): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (to_k): Linear(in_features=2048, out_features=1536, bias=True)\n",
      "            (to_v): Linear(in_features=2048, out_features=1536, bias=True)\n",
      "            (to_out): ModuleList(\n",
      "              (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "              (1): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
      "          (ff): FeedForward(\n",
      "            (net): ModuleList(\n",
      "              (0): GELU(\n",
      "                (proj): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "              )\n",
      "              (1): Dropout(p=0.2, inplace=False)\n",
      "              (2): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "              (3): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (final_dropout): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "        (27): BasicTransformerBlock(\n",
      "          (norm1): AdaLayerNorm(\n",
      "            (silu): SiLU()\n",
      "            (linear): Linear(in_features=1536, out_features=3072, bias=True)\n",
      "            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
      "          )\n",
      "          (attn1): Attention(\n",
      "            (to_q): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (to_k): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (to_v): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (to_out): ModuleList(\n",
      "              (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "              (1): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
      "          (ff): FeedForward(\n",
      "            (net): ModuleList(\n",
      "              (0): GELU(\n",
      "                (proj): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "              )\n",
      "              (1): Dropout(p=0.2, inplace=False)\n",
      "              (2): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "              (3): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (final_dropout): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "        (28): BasicTransformerBlock(\n",
      "          (norm1): AdaLayerNorm(\n",
      "            (silu): SiLU()\n",
      "            (linear): Linear(in_features=1536, out_features=3072, bias=True)\n",
      "            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
      "          )\n",
      "          (attn1): Attention(\n",
      "            (to_q): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (to_k): Linear(in_features=2048, out_features=1536, bias=True)\n",
      "            (to_v): Linear(in_features=2048, out_features=1536, bias=True)\n",
      "            (to_out): ModuleList(\n",
      "              (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "              (1): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
      "          (ff): FeedForward(\n",
      "            (net): ModuleList(\n",
      "              (0): GELU(\n",
      "                (proj): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "              )\n",
      "              (1): Dropout(p=0.2, inplace=False)\n",
      "              (2): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "              (3): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (final_dropout): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "        (29): BasicTransformerBlock(\n",
      "          (norm1): AdaLayerNorm(\n",
      "            (silu): SiLU()\n",
      "            (linear): Linear(in_features=1536, out_features=3072, bias=True)\n",
      "            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
      "          )\n",
      "          (attn1): Attention(\n",
      "            (to_q): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (to_k): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (to_v): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (to_out): ModuleList(\n",
      "              (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "              (1): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
      "          (ff): FeedForward(\n",
      "            (net): ModuleList(\n",
      "              (0): GELU(\n",
      "                (proj): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "              )\n",
      "              (1): Dropout(p=0.2, inplace=False)\n",
      "              (2): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "              (3): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (final_dropout): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "        (30): BasicTransformerBlock(\n",
      "          (norm1): AdaLayerNorm(\n",
      "            (silu): SiLU()\n",
      "            (linear): Linear(in_features=1536, out_features=3072, bias=True)\n",
      "            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
      "          )\n",
      "          (attn1): Attention(\n",
      "            (to_q): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (to_k): Linear(in_features=2048, out_features=1536, bias=True)\n",
      "            (to_v): Linear(in_features=2048, out_features=1536, bias=True)\n",
      "            (to_out): ModuleList(\n",
      "              (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "              (1): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
      "          (ff): FeedForward(\n",
      "            (net): ModuleList(\n",
      "              (0): GELU(\n",
      "                (proj): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "              )\n",
      "              (1): Dropout(p=0.2, inplace=False)\n",
      "              (2): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "              (3): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (final_dropout): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "        (31): BasicTransformerBlock(\n",
      "          (norm1): AdaLayerNorm(\n",
      "            (silu): SiLU()\n",
      "            (linear): Linear(in_features=1536, out_features=3072, bias=True)\n",
      "            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
      "          )\n",
      "          (attn1): Attention(\n",
      "            (to_q): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (to_k): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (to_v): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "            (to_out): ModuleList(\n",
      "              (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "              (1): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=False)\n",
      "          (ff): FeedForward(\n",
      "            (net): ModuleList(\n",
      "              (0): GELU(\n",
      "                (proj): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "              )\n",
      "              (1): Dropout(p=0.2, inplace=False)\n",
      "              (2): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "              (3): Dropout(p=0.2, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (final_dropout): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm_out): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
      "      (proj_out_1): Linear(in_features=1536, out_features=3072, bias=True)\n",
      "      (proj_out_2): Linear(in_features=1536, out_features=1024, bias=True)\n",
      "    )\n",
      "    (state_encoder): CategorySpecificMLP(\n",
      "      (layer1): CategorySpecificLinear()\n",
      "      (layer2): CategorySpecificLinear()\n",
      "    )\n",
      "    (action_encoder): MultiEmbodimentActionEncoder(\n",
      "      (W1): CategorySpecificLinear()\n",
      "      (W2): CategorySpecificLinear()\n",
      "      (W3): CategorySpecificLinear()\n",
      "      (pos_encoding): SinusoidalPositionalEncoding()\n",
      "    )\n",
      "    (action_decoder): CategorySpecificMLP(\n",
      "      (layer1): CategorySpecificLinear()\n",
      "      (layer2): CategorySpecificLinear()\n",
      "    )\n",
      "    (vlln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "    (position_embedding): Embedding(1024, 1536)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "policy = Gr00tPolicy(\n",
    "    model_path=MODEL_PATH,\n",
    "    embodiment_tag=EmbodimentTag(EMBODIMENT_TAG),\n",
    "    device=device,\n",
    "    strict=True,\n",
    ")\n",
    "\n",
    "# print out the policy model architecture\n",
    "print(policy.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First this requires user to check which embodiment tags are used to pretrained the `Gr00tPolicy` pretrained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['video', 'state', 'action', 'language'])\n",
      "video ModalityConfig(delta_indices=[0], modality_keys=['ego_view_bg_crop_pad_res256_freq20'], sin_cos_embedding_keys=None, mean_std_embedding_keys=None, action_configs=None)\n",
      "state ModalityConfig(delta_indices=[0], modality_keys=['left_arm', 'right_arm', 'left_hand', 'right_hand', 'waist'], sin_cos_embedding_keys=['left_arm', 'right_arm', 'left_hand', 'right_hand', 'waist'], mean_std_embedding_keys=None, action_configs=None)\n",
      "action ModalityConfig(delta_indices=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], modality_keys=['left_arm', 'right_arm', 'left_hand', 'right_hand', 'waist'], sin_cos_embedding_keys=None, mean_std_embedding_keys=None, action_configs=[ActionConfig(rep=<ActionRepresentation.RELATIVE: 'relative'>, type=<ActionType.NON_EEF: 'non_eef'>, format=<ActionFormat.DEFAULT: 'default'>, state_key=None), ActionConfig(rep=<ActionRepresentation.RELATIVE: 'relative'>, type=<ActionType.NON_EEF: 'non_eef'>, format=<ActionFormat.DEFAULT: 'default'>, state_key=None), ActionConfig(rep=<ActionRepresentation.RELATIVE: 'relative'>, type=<ActionType.NON_EEF: 'non_eef'>, format=<ActionFormat.DEFAULT: 'default'>, state_key=None), ActionConfig(rep=<ActionRepresentation.RELATIVE: 'relative'>, type=<ActionType.NON_EEF: 'non_eef'>, format=<ActionFormat.DEFAULT: 'default'>, state_key=None), ActionConfig(rep=<ActionRepresentation.ABSOLUTE: 'absolute'>, type=<ActionType.NON_EEF: 'non_eef'>, format=<ActionFormat.DEFAULT: 'default'>, state_key=None)])\n",
      "language ModalityConfig(delta_indices=[0], modality_keys=['task'], sin_cos_embedding_keys=None, mean_std_embedding_keys=None, action_configs=None)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "modality_config = policy.get_modality_config()\n",
    "\n",
    "print(modality_config.keys())\n",
    "\n",
    "for key, value in modality_config.items():\n",
    "    if isinstance(value, np.ndarray):\n",
    "        print(key, value.shape)\n",
    "    else:\n",
    "        print(key, value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "dataset = LeRobotEpisodeLoader(\n",
    "    dataset_path=DATASET_PATH,\n",
    "    modality_configs=modality_config,\n",
    "    video_backend=\"torchcodec\",\n",
    "    video_backend_kwargs=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print out a single data and visualize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task\n",
      "VLAStepData(images={'ego_view_bg_crop_pad_res256_freq20': [array([[[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]]], dtype=uint8)]}, states={'left_arm': array([[-0.01147083,  0.12207967,  0.04229397, -2.1       , -0.01441445,\n",
      "        -0.03013532, -0.00384387]], dtype=float32), 'right_arm': array([[ 6.7468253e-03, -9.0524264e-02,  8.1401030e-03, -2.0999999e+00,\n",
      "        -2.2280285e-02,  1.4037349e-02,  1.0467973e-03]], dtype=float32), 'left_hand': array([[0.00067388, 0.00076318, 0.00084776, 0.00069391, 0.00075118,\n",
      "        0.00485883]], dtype=float32), 'right_hand': array([[0.00187319, 0.00216486, 0.002383  , 0.00182536, 0.02100907,\n",
      "        0.01359221]], dtype=float32), 'waist': array([[0., 0., 0.]], dtype=float32)}, actions={'left_arm': array([[-1.07706683e-02,  1.04934342e-01,  4.15936075e-02,\n",
      "        -2.09762359e+00, -1.78164821e-02, -3.33876684e-02,\n",
      "        -9.85330623e-03],\n",
      "       [-7.91579206e-03,  9.78180766e-02,  3.57741937e-02,\n",
      "        -2.09809613e+00, -1.47751635e-02, -2.78760642e-02,\n",
      "        -1.44522488e-02],\n",
      "       [-5.48170460e-03,  8.97774324e-02,  3.09967920e-02,\n",
      "        -2.09854674e+00, -1.20841814e-02, -2.37416662e-02,\n",
      "        -1.86673086e-02],\n",
      "       [-3.23207909e-03,  8.14548284e-02,  2.66160704e-02,\n",
      "        -2.09891415e+00, -9.46781412e-03, -2.01521628e-02,\n",
      "        -2.27768123e-02],\n",
      "       [-1.03295769e-03,  7.30429515e-02,  2.23631486e-02,\n",
      "        -2.09919643e+00, -6.85977051e-03, -1.68003943e-02,\n",
      "        -2.69124769e-02],\n",
      "       [ 1.18218130e-03,  6.45925477e-02,  1.81263909e-02,\n",
      "        -2.09939837e+00, -4.25067544e-03, -1.35736447e-02,\n",
      "        -3.11342310e-02],\n",
      "       [ 4.01225174e-03,  6.74696192e-02,  9.50461067e-03,\n",
      "        -2.08267713e+00,  1.30771240e-02, -2.74863634e-02,\n",
      "        -6.21405011e-03],\n",
      "       [ 5.70485229e-03,  6.30450994e-02,  5.21468371e-03,\n",
      "        -2.07986212e+00,  1.71772093e-02, -2.82865558e-02,\n",
      "        -4.63431142e-03],\n",
      "       [ 7.39730336e-03,  5.89018911e-02,  1.03944237e-03,\n",
      "        -2.07728100e+00,  2.08109617e-02, -2.90521868e-02,\n",
      "        -3.36559187e-03],\n",
      "       [ 9.12183523e-03,  5.53131402e-02, -2.71213218e-03,\n",
      "        -2.07524967e+00,  2.33213492e-02, -2.95806658e-02,\n",
      "        -2.84210150e-03],\n",
      "       [ 1.09992158e-02,  5.22728190e-02, -6.07222738e-03,\n",
      "        -2.07378101e+00,  2.45457850e-02, -2.98420452e-02,\n",
      "        -3.15860449e-03],\n",
      "       [ 1.30662713e-02,  4.97582406e-02, -9.05391015e-03,\n",
      "        -2.07285023e+00,  2.45056916e-02, -2.98503265e-02,\n",
      "        -4.26775124e-03],\n",
      "       [ 1.52062085e-02,  4.77021039e-02, -1.16503108e-02,\n",
      "        -2.07234025e+00,  2.35690493e-02, -2.96972468e-02,\n",
      "        -5.86959068e-03],\n",
      "       [ 1.73108242e-02,  4.59530912e-02, -1.39242262e-02,\n",
      "        -2.07209134e+00,  2.21695453e-02, -2.94920858e-02,\n",
      "        -7.65797542e-03],\n",
      "       [ 1.92511044e-02,  4.44102585e-02, -1.58636365e-02,\n",
      "        -2.07202554e+00,  2.06067599e-02, -2.93277409e-02,\n",
      "        -9.35161300e-03],\n",
      "       [ 2.07126401e-02,  4.32274193e-02, -1.72182433e-02,\n",
      "        -2.07203341e+00,  1.92976594e-02, -2.92817187e-02,\n",
      "        -1.06464978e-02]], dtype=float32), 'right_arm': array([[ 2.21627089e-03, -9.37728584e-02, -3.97849903e-02,\n",
      "        -2.06814218e+00, -8.56739208e-02,  5.79283610e-02,\n",
      "        -5.77253327e-02],\n",
      "       [ 4.30219770e-05, -1.04490615e-01, -7.72047043e-02,\n",
      "        -2.04360056e+00, -1.49008811e-01,  9.11731347e-02,\n",
      "        -1.09504737e-01],\n",
      "       [-3.20656179e-03, -1.13106631e-01, -1.10705756e-01,\n",
      "        -2.02076697e+00, -2.06495166e-01,  1.24586381e-01,\n",
      "        -1.55423895e-01],\n",
      "       [-7.43041234e-03, -1.21159330e-01, -1.41850203e-01,\n",
      "        -1.99755442e+00, -2.60498047e-01,  1.59194991e-01,\n",
      "        -1.97590396e-01],\n",
      "       [-1.24604637e-02, -1.29517660e-01, -1.71688780e-01,\n",
      "        -1.97291112e+00, -3.12480599e-01,  1.95620000e-01,\n",
      "        -2.37292811e-01],\n",
      "       [-1.81944016e-02, -1.38630152e-01, -2.00705543e-01,\n",
      "        -1.94634569e+00, -3.63152742e-01,  2.34135047e-01,\n",
      "        -2.75149286e-01],\n",
      "       [ 8.93425345e-02, -4.26527768e-01, -1.26975700e-01,\n",
      "        -1.93934417e+00,  9.87892970e-02,  2.32846573e-01,\n",
      "        -2.87245121e-02],\n",
      "       [ 9.94114280e-02, -4.84456897e-01, -1.19981058e-01,\n",
      "        -1.92916310e+00,  1.56401128e-01,  2.58826792e-01,\n",
      "        -5.84780565e-03],\n",
      "       [ 1.05915502e-01, -5.33586025e-01, -1.09268822e-01,\n",
      "        -1.92013252e+00,  2.07781017e-01,  2.81349778e-01,\n",
      "         1.95781458e-02],\n",
      "       [ 1.05701104e-01, -5.70159733e-01, -9.94966477e-02,\n",
      "        -1.91462445e+00,  2.47525021e-01,  2.97567725e-01,\n",
      "         4.42939252e-02],\n",
      "       [ 9.88279358e-02, -5.93690813e-01, -8.82906541e-02,\n",
      "        -1.91157436e+00,  2.70735234e-01,  3.12278003e-01,\n",
      "         7.16265067e-02],\n",
      "       [ 8.50292370e-02, -6.07675433e-01, -8.17918107e-02,\n",
      "        -1.91183877e+00,  2.81771719e-01,  3.23118627e-01,\n",
      "         1.02626033e-01],\n",
      "       [ 6.71502277e-02, -6.18867874e-01, -7.60968029e-02,\n",
      "        -1.91468072e+00,  2.81581402e-01,  3.40363264e-01,\n",
      "         1.35024220e-01],\n",
      "       [ 4.65002768e-02, -6.27704620e-01, -7.52606243e-02,\n",
      "        -1.91926014e+00,  2.75164723e-01,  3.59271049e-01,\n",
      "         1.67061225e-01],\n",
      "       [ 2.45882217e-02, -6.33134723e-01, -7.88275972e-02,\n",
      "        -1.92399108e+00,  2.61009604e-01,  3.81247491e-01,\n",
      "         1.98397055e-01],\n",
      "       [ 5.03588608e-03, -6.39530361e-01, -8.39274526e-02,\n",
      "        -1.92642546e+00,  2.47624025e-01,  4.06113267e-01,\n",
      "         2.24670306e-01]], dtype=float32), 'left_hand': array([[-1.5, -1.5, -1.5, -1.5, -3. ,  3. ],\n",
      "       [-1.5, -1.5, -1.5, -1.5, -3. ,  3. ],\n",
      "       [-1.5, -1.5, -1.5, -1.5, -3. ,  3. ],\n",
      "       [-1.5, -1.5, -1.5, -1.5, -3. ,  3. ],\n",
      "       [-1.5, -1.5, -1.5, -1.5, -3. ,  3. ],\n",
      "       [-1.5, -1.5, -1.5, -1.5, -3. ,  3. ],\n",
      "       [-1.5, -1.5, -1.5, -1.5, -3. ,  3. ],\n",
      "       [-1.5, -1.5, -1.5, -1.5, -3. ,  3. ],\n",
      "       [-1.5, -1.5, -1.5, -1.5, -3. ,  3. ],\n",
      "       [-1.5, -1.5, -1.5, -1.5, -3. ,  3. ],\n",
      "       [-1.5, -1.5, -1.5, -1.5, -3. ,  3. ],\n",
      "       [-1.5, -1.5, -1.5, -1.5, -3. ,  3. ],\n",
      "       [-1.5, -1.5, -1.5, -1.5, -3. ,  3. ],\n",
      "       [-1.5, -1.5, -1.5, -1.5, -3. ,  3. ],\n",
      "       [-1.5, -1.5, -1.5, -1.5, -3. ,  3. ],\n",
      "       [-1.5, -1.5, -1.5, -1.5, -3. ,  3. ]], dtype=float32), 'right_hand': array([[-1.5, -1.5, -1.5, -1.5, -3. ,  3. ],\n",
      "       [-1.5, -1.5, -1.5, -1.5, -3. ,  3. ],\n",
      "       [-1.5, -1.5, -1.5, -1.5, -3. ,  3. ],\n",
      "       [-1.5, -1.5, -1.5, -1.5, -3. ,  3. ],\n",
      "       [-1.5, -1.5, -1.5, -1.5, -3. ,  3. ],\n",
      "       [-1.5, -1.5, -1.5, -1.5, -3. ,  3. ],\n",
      "       [-1.5, -1.5, -1.5, -1.5, -3. ,  3. ],\n",
      "       [-1.5, -1.5, -1.5, -1.5, -3. ,  3. ],\n",
      "       [-1.5, -1.5, -1.5, -1.5, -3. ,  3. ],\n",
      "       [-1.5, -1.5, -1.5, -1.5, -3. ,  3. ],\n",
      "       [-1.5, -1.5, -1.5, -1.5, -3. ,  3. ],\n",
      "       [-1.5, -1.5, -1.5, -1.5, -3. ,  3. ],\n",
      "       [-1.5, -1.5, -1.5, -1.5, -3. ,  3. ],\n",
      "       [-1.5, -1.5, -1.5, -1.5, -3. ,  3. ],\n",
      "       [-1.5, -1.5, -1.5, -1.5, -3. ,  3. ],\n",
      "       [-1.5, -1.5, -1.5, -1.5, -3. ,  3. ]], dtype=float32), 'waist': array([[0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.]], dtype=float32)}, text='pick the pear from the counter and place it in the plate', embodiment=<EmbodimentTag.GR1: 'gr1'>, is_demonstration=False, metadata={})\n",
      "\n",
      "\n",
      "====================================\n",
      "Images:\n",
      "     ego_view_bg_crop_pad_res256_freq20 1 x (256, 256, 3)\n",
      "\n",
      "States:\n",
      "     left_arm (1, 7)\n",
      "     right_arm (1, 7)\n",
      "     left_hand (1, 6)\n",
      "     right_hand (1, 6)\n",
      "     waist (1, 3)\n",
      "\n",
      "Actions:\n",
      "     left_arm (16, 7)\n",
      "     right_arm (16, 7)\n",
      "     left_hand (16, 6)\n",
      "     right_hand (16, 6)\n",
      "     waist (16, 3)\n",
      "\n",
      "Task:  pick the pear from the counter and place it in the plate\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "episode_data = dataset[0]\n",
    "step_data = extract_step_data(\n",
    "    episode_data, step_index=0, modality_configs=modality_config, embodiment_tag=EmbodimentTag(EMBODIMENT_TAG), allow_padding=False\n",
    ")\n",
    "\n",
    "print(step_data)\n",
    "\n",
    "print(\"\\n\\n====================================\")\n",
    "print(\"Images:\")\n",
    "for img_key in step_data.images:\n",
    "    print(\" \" * 4, img_key, f\"{len(step_data.images[img_key])} x {step_data.images[img_key][0].shape}\")\n",
    "\n",
    "print(\"\\nStates:\")\n",
    "for state_key in step_data.states:\n",
    "    print(\" \" * 4, state_key, step_data.states[state_key].shape)\n",
    "\n",
    "print(\"\\nActions:\")\n",
    "for action_key in step_data.actions:\n",
    "    print(\" \" * 4, action_key, step_data.actions[action_key].shape)\n",
    "\n",
    "print(\"\\nTask: \", step_data.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot just the \"right arm\" state and action data and see how it looks like. Also show the images of the right hand state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "episode_index = 0\n",
    "max_steps = 400\n",
    "joint_name = \"right_arm\"\n",
    "image_key = \"ego_view_bg_crop_pad_res256_freq20\"\n",
    "\n",
    "state_joints_across_time = []\n",
    "gt_action_joints_across_time = []\n",
    "images = []\n",
    "\n",
    "sample_images = 6\n",
    "episode_data = dataset[episode_index]\n",
    "print(len(episode_data))\n",
    "\n",
    "for step_count in range(max_steps):\n",
    "    data_point = extract_step_data(\n",
    "        episode_data, step_index=step_count, modality_configs=modality_config, embodiment_tag=EmbodimentTag(EMBODIMENT_TAG), allow_padding=False\n",
    "    )\n",
    "    state_joints = data_point.states[joint_name][0]\n",
    "    gt_action_joints = data_point.actions[joint_name][0]\n",
    "\n",
    "    state_joints_across_time.append(state_joints)\n",
    "    gt_action_joints_across_time.append(gt_action_joints)\n",
    "\n",
    "    # We can also get the image data\n",
    "    if step_count % (max_steps // sample_images) == 0:\n",
    "        image = data_point.images[image_key][0]\n",
    "        images.append(image)\n",
    "\n",
    "# Size is (max_steps, num_joints)\n",
    "state_joints_across_time = np.array(state_joints_across_time)\n",
    "gt_action_joints_across_time = np.array(gt_action_joints_across_time)\n",
    "\n",
    "\n",
    "# Plot the joint angles across time\n",
    "num_joints = state_joints_across_time.shape[1]\n",
    "fig, axes = plt.subplots(nrows=num_joints, ncols=1, figsize=(8, 2*num_joints))\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.plot(state_joints_across_time[:, i], label=\"state joints\")\n",
    "    ax.plot(gt_action_joints_across_time[:, i], label=\"gt action joints\")\n",
    "    ax.set_title(f\"Joint {i}\")\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plot the images in a row\n",
    "fig, axes = plt.subplots(nrows=1, ncols=sample_images, figsize=(16, 4))\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.imshow(images[i])\n",
    "    ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run the policy from the pretrained checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = {\n",
    "    \"video\": {k: np.stack(step_data.images[k])[None] for k in step_data.images},  # stach images and add batch dimension\n",
    "    \"state\": {k: step_data.states[k][None] for k in step_data.states},  # add batch dimension\n",
    "    \"action\": {k: step_data.actions[k][None] for k in step_data.actions},  # add batch dimension\n",
    "    \"language\": {\n",
    "        modality_config[\"language\"].modality_keys[0]: [[step_data.text]],  # add time and batch dimension\n",
    "    }\n",
    "}\n",
    "predicted_action, info = policy.get_action(observation)\n",
    "for key, value in predicted_action.items():\n",
    "    print(key, value.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more details on the policy (e.g. expected input and output), please refer to the [policy documentation](policy.md)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gr00t (3.10.19)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
